import random
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.preprocessing import MinMaxScaler
import networkx as nx
import json
import os

# ================== é…ç½®å‚æ•° ==================
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

TOTAL_TRACES = 8000  # æ€»è°ƒç”¨é“¾æ•°é‡
LOG_MULTIPLIER = 5  # æ—¥å¿—æ•°é‡ç›¸å¯¹äºtraceçš„å€æ•°
OUTPUT_DIR = "dataset"  # è¾“å‡ºç›®å½•

# å¾®æœåŠ¡æ¶æ„å®šä¹‰
SERVICES = [
    'ts-order-service',
    'ts-user-service',
    'ts-auth-service',
    'ts-ticket-service',
    'ts-station-service',
    'ts-payment-service',
    'ts-config-service'
]


# ================== æ•°æ®ç”Ÿæˆæ ¸å¿ƒå‡½æ•° ==================
def generate_base_traces(num):
    """ç”ŸæˆåŸºç¡€è°ƒç”¨é“¾æ•°æ®"""
    traces = []
    for _ in range(num):
        trace_id = f"trace-{random.randint(100000, 999999)}"
        duration = abs(int(np.random.normal(300, 50)))
        timestamp = datetime.now() - timedelta(seconds=random.randint(0, 86400))
        service = random.choice(SERVICES)

        traces.append({
            "trace_id": trace_id,
            "service": service,
            "duration_ms": duration,
            "timestamp": timestamp,
            "status": "success",
            "call_path": generate_call_path(),
            "anomaly_type": "normal"
        })
    return pd.DataFrame(traces)


def generate_call_path():
    """ç”Ÿæˆéšæœºè°ƒç”¨è·¯å¾„"""
    path_length = random.randint(3, min(5, len(SERVICES)))
    return "->".join(random.sample(SERVICES, path_length))


class AnomalyInjector:
    """å¼‚å¸¸æ³¨å…¥ç³»ç»Ÿï¼ˆ6ç§å¼‚å¸¸ç±»å‹ï¼‰"""

    @staticmethod
    def inject_anomalies(traces_df, anomaly_ratio=0.2):
        total_samples = len(traces_df)
        normal_count = int(total_samples * (1 - anomaly_ratio))
        total_anomalies = total_samples - normal_count

        # å®šä¹‰å¼‚å¸¸ç±»å‹åˆ†å¸ƒ
        anomaly_dist = {
            'timeout': int(total_anomalies * 0.25),
            'cascade_failure': int(total_anomalies * 0.20),
            'high_error_rate': int(total_anomalies * 0.15),
            'infinite_loop': int(total_anomalies * 0.10),
            'resource_exhaustion': int(total_anomalies * 0.15),
            'illegal_argument': int(total_anomalies * 0.15)
        }

        # å¤„ç†å››èˆäº”å…¥è¯¯å·®
        diff = total_anomalies - sum(anomaly_dist.values())
        anomaly_dist['timeout'] += diff

        # åˆå§‹åŒ–æ‰€æœ‰æ ·æœ¬ä¸ºæ­£å¸¸
        traces_df['anomaly_type'] = 'normal'
        anomaly_indices = []

        candidates = traces_df.index.difference(anomaly_indices)
        selected = np.random.choice(list(candidates), size=anomaly_dist['timeout'], replace=False)
        traces_df.loc[selected, 'anomaly_type'] = 'timeout'
        traces_df.loc[selected, 'duration_ms'] *= 10  # 10å€å»¶è¿Ÿ
        anomaly_indices.extend(selected)

        # ============= 2. æ³¨å…¥çº§è”æ•…éšœ =============
        candidates = traces_df.index.difference(anomaly_indices)
        selected = np.random.choice(list(candidates), size=anomaly_dist['cascade_failure'], replace=False)
        traces_df.loc[selected, 'anomaly_type'] = 'cascade_failure'
        traces_df.loc[selected, 'status'] = 'failed'
        # çº§è”æ•…éšœçš„ç‰¹å¾è·¯å¾„
        cascade_path = "ts-auth-service->ts-order-service->ts-payment-service"
        traces_df.loc[selected, 'call_path'] = cascade_path
        anomaly_indices.extend(selected)

        # ============= 3. æ³¨å…¥é«˜é”™è¯¯ç‡ =============
        candidates = traces_df.index.difference(anomaly_indices)
        selected = np.random.choice(list(candidates), size=anomaly_dist['high_error_rate'], replace=False)
        traces_df.loc[selected, 'anomaly_type'] = 'high_error_rate'
        traces_df.loc[selected, 'status'] = 'failed'
        # é›†ä¸­åœ¨ticket-service
        traces_df.loc[selected, 'service'] = 'ts-ticket-service'
        anomaly_indices.extend(selected)

        # ============= 4. æ³¨å…¥æ— é™å¾ªç¯ =============
        candidates = traces_df.index.difference(anomaly_indices)
        selected = np.random.choice(list(candidates), size=anomaly_dist['infinite_loop'], replace=False)
        traces_df.loc[selected, 'anomaly_type'] = 'infinite_loop'
        traces_df.loc[selected, 'duration_ms'] = 30000  # å›ºå®š30ç§’å»¶è¿Ÿ
        # æ·»åŠ å¾ªç¯ç‰¹å¾è·¯å¾„
        loop_path = "ts-user-service->ts-user-service->ts-user-service"
        traces_df.loc[selected, 'call_path'] = loop_path
        anomaly_indices.extend(selected)

        # ============= 5. æ³¨å…¥èµ„æºè€—å°½ =============
        candidates = traces_df.index.difference(anomaly_indices)
        selected = np.random.choice(list(candidates), size=anomaly_dist['resource_exhaustion'], replace=False)
        traces_df.loc[selected, 'anomaly_type'] = 'resource_exhaustion'
        traces_df.loc[selected, 'duration_ms'] = traces_df.loc[selected, 'duration_ms'] * 20
        traces_df.loc[selected, 'status'] = 'failed'
        # æ·»åŠ å†…å­˜é”™è¯¯ç‰¹å¾
        traces_df.loc[selected, 'service'] = 'ts-config-service'
        anomaly_indices.extend(selected)

        # ============= 6. æ³¨å…¥éæ³•å‚æ•° =============
        candidates = traces_df.index.difference(anomaly_indices)
        selected = np.random.choice(list(candidates), size=anomaly_dist['illegal_argument'], replace=False)
        traces_df.loc[selected, 'anomaly_type'] = 'illegal_argument'
        traces_df.loc[selected, 'status'] = 'failed'
        traces_df.loc[selected, 'duration_ms'] = 100  # å¿«é€Ÿå¤±è´¥
        # å‚æ•°é”™è¯¯ç‰¹å¾è·¯å¾„
        arg_path = "ts-order-service->ts-station-service->ts-ticket-service"
        traces_df.loc[selected, 'call_path'] = arg_path
        anomaly_indices.extend(selected)

        return traces_df


def generate_logs_with_anomalies(traces_df):
    """ç”Ÿæˆä¸å¼‚å¸¸å…³è”çš„æ—¥å¿—æ•°æ®"""
    logs = []
    log_templates = {
        "normal": "{service} processed request in {duration}ms",
        "timeout": "{service} request timeout after {duration}ms | trace_id={trace_id}",
        "cascade_failure": "{service} failed due to dependency service breakdown | trace_id={trace_id}",
        "high_error_rate": "{service} internal server error (code:5XX) | trace_id={trace_id}",
        "infinite_loop": "{service} potential infinite loop detected | duration={duration}ms",
        "resource_exhaustion": "{service} resource exhausted (memory/CPU) | trace_id={trace_id}",
        "illegal_argument": "{service} received illegal argument: code={error_code}"
    }

    for _, trace in traces_df.iterrows():
        log_count = random.randint(1, 3)
        for _ in range(log_count):
            try:
                message = log_templates[trace['anomaly_type']].format(
                    service=trace['service'],
                    duration=trace['duration_ms'],
                    trace_id=trace['trace_id'],
                    error_code=random.randint(400, 499)  # ç”¨äºillegal_argument
                )
            except KeyError:
                message = f"{trace['service']} unknown anomaly occurred"

            logs.append({
                "trace_id": trace['trace_id'],
                "service": trace['service'],
                "timestamp": trace['timestamp'] + timedelta(milliseconds=random.randint(0, trace['duration_ms'])),
                "level": "ERROR" if trace['anomaly_type'] != 'normal' else
                random.choices(["INFO", "WARN"], weights=[0.9, 0.1])[0],
                "message": message,
                "anomaly_type": trace['anomaly_type']
            })
    return pd.DataFrame(logs)


# ================== ç‰¹å¾å·¥ç¨‹ ==================
def extract_trace_features(traces_df):
    """æå–è°ƒç”¨é“¾ç‰¹å¾"""
    # è°ƒç”¨è·¯å¾„ç‰¹å¾
    traces_df['path_length'] = traces_df['call_path'].apply(lambda x: len(x.split('->')))

    # æ—¶é—´ç‰¹å¾
    traces_df['timestamp'] = pd.to_datetime(traces_df['timestamp'])
    traces_df['hour'] = traces_df['timestamp'].dt.hour

    # æœåŠ¡çƒ­ç‚¹ç¼–ç 
    service_dummies = pd.get_dummies(traces_df['service'], prefix='service')
    traces_df = pd.concat([traces_df, service_dummies], axis=1)

    # æ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–
    scaler = MinMaxScaler()
    traces_df[['duration_norm', 'path_length_norm']] = scaler.fit_transform(
        traces_df[['duration_ms', 'path_length']])

    return traces_df


def build_service_graph(traces_df):
    """æ„å»ºæœåŠ¡è°ƒç”¨å…³ç³»å›¾ï¼ˆå¸¦å®Œæ•´å±æ€§ï¼‰"""
    G = nx.DiGraph()

    # ============= 1. èŠ‚ç‚¹åˆå§‹åŒ– =============
    # é¢„å…ˆè®¡ç®—æœåŠ¡çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯
    service_stats = traces_df.groupby('service').agg({
        'duration_ms': ['mean', 'max', 'min'],
        'status': lambda x: (x == 'failed').mean()  # é”™è¯¯ç‡
    }).reset_index()
    service_stats.columns = ['service', 'avg_duration', 'max_duration', 'min_duration', 'error_rate']

    # æ·»åŠ èŠ‚ç‚¹åŠåŸºç¡€å±æ€§
    for service in SERVICES:
        stats = service_stats[service_stats['service'] == service].iloc[0] if service in service_stats[
            'service'].values else {
            'avg_duration': 300, 'max_duration': 500, 'min_duration': 100, 'error_rate': 0.01
        }
        G.add_node(service,
                   service_type=("backend" if "ts-" in service else "gateway"),
                   avg_cpu=random.uniform(10, 80),  # æ¨¡æ‹ŸCPUä½¿ç”¨ç‡
                   avg_mem=random.uniform(20, 90),  # æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨ç‡
                   **stats.to_dict())

    # ============= 2. è¾¹æ„å»º =============
    # è®¡ç®—è¾¹çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¿®å¤æ‹¬å·é—®é¢˜ï¼‰
    edge_records = []
    for _, row in traces_df.iterrows():
        nodes = row['call_path'].split('->')
        for i in range(len(nodes) - 1):
            edge_records.append({
                'source': nodes[i],
                'target': nodes[i + 1],
                'trace_id': row['trace_id'],
                'duration': row['duration_ms'],
                'status': row['status']
            })

    edge_stats = pd.DataFrame(edge_records)

    if not edge_stats.empty:
        edge_agg = edge_stats.groupby(['source', 'target']).agg({
            'duration': ['mean', 'count'],
            'status': lambda x: (x == 'failed').mean()
        }).reset_index()
        edge_agg.columns = ['source', 'target', 'avg_duration', 'call_count', 'error_rate']

        # æ·»åŠ å¸¦å±æ€§çš„è¾¹
        for _, edge in edge_agg.iterrows():
            G.add_edge(edge['source'], edge['target'],
                       avg_duration=edge['avg_duration'],
                       call_count=edge['call_count'],
                       error_rate=edge['error_rate'],
                       weight=edge['call_count'] / edge_agg['call_count'].max())  # å½’ä¸€åŒ–æƒé‡

    # ============= 3. å›¾çº§åˆ«ç‰¹å¾ =============
    # è®¡ç®—ä¸­å¿ƒæ€§æŒ‡æ ‡
    for node in G.nodes():
        G.nodes[node]['degree_centrality'] = nx.degree_centrality(G).get(node, 0)
        G.nodes[node]['betweenness_centrality'] = nx.betweenness_centrality(G).get(node, 0)
        G.nodes[node]['closeness_centrality'] = nx.closeness_centrality(G).get(node, 0)

    # æ·»åŠ å›¾çº§åˆ«å…ƒæ•°æ®
    G.graph['created_at'] = datetime.now().isoformat()
    G.graph['total_traces'] = len(traces_df)
    G.graph['avg_duration'] = traces_df['duration_ms'].mean()

    return G


# ================== ä¸»æµç¨‹ ==================
def main():
    print("ğŸš€ å¼€å§‹ç”Ÿæˆæ•°æ®é›†...")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # 1. ç”ŸæˆåŸºç¡€è°ƒç”¨é“¾æ•°æ®
    traces_df = generate_base_traces(TOTAL_TRACES)
    print(f"âœ… ç”ŸæˆåŸºç¡€è°ƒç”¨é“¾æ•°æ®å®Œæˆï¼ˆå…±{len(traces_df)}æ¡ï¼‰")

    # 2. æ³¨å…¥å¼‚å¸¸ï¼ˆ20%æ¯”ä¾‹ï¼‰
    traces_df = AnomalyInjector.inject_anomalies(traces_df)
    normal_ratio = len(traces_df[traces_df['anomaly_type'] == 'normal']) / len(traces_df)
    print(f"âœ… å¼‚å¸¸æ³¨å…¥å®Œæˆï¼ˆæ­£å¸¸æ•°æ®æ¯”ä¾‹ï¼š{normal_ratio:.1%}ï¼‰")

    # 3. ç”Ÿæˆå…³è”æ—¥å¿—
    logs_df = generate_logs_with_anomalies(traces_df)
    print(f"âœ… ç”Ÿæˆå…³è”æ—¥å¿—å®Œæˆï¼ˆå…±{len(logs_df)}æ¡ï¼‰")

    # 4. ç‰¹å¾å·¥ç¨‹
    traces_df = extract_trace_features(traces_df)
    print("âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ")

    # 5. æ„å»ºæœåŠ¡è°ƒç”¨å›¾
    service_graph = build_service_graph(traces_df)
    print("âœ… æœåŠ¡è°ƒç”¨å›¾æ„å»ºå®Œæˆ")

    # 6. ä¿å­˜æ•°æ®é›†
    traces_df.to_csv(f"{OUTPUT_DIR}/traces_processed.csv", index=False)
    logs_df.to_csv(f"{OUTPUT_DIR}/logs_processed.csv", index=False)
    nx.write_gexf(service_graph, f"{OUTPUT_DIR}/service_graph.gexf")

    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "total_traces": len(traces_df),
        "normal_ratio": normal_ratio,
        "anomaly_distribution": traces_df['anomaly_type'].value_counts().to_dict(),
        "log_level_distribution": logs_df['level'].value_counts().to_dict()
    }
    with open(f"{OUTPUT_DIR}/dataset_stats.json", 'w') as f:
        json.dump(stats, f, indent=2)

    print(f"""
ğŸ‰ æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼ä¿å­˜åœ¨ {OUTPUT_DIR} ç›®å½•ä¸‹ï¼š
  - traces_processed.csvï¼šå¤„ç†åçš„è°ƒç”¨é“¾æ•°æ®
  - logs_processed.csvï¼šå…³è”æ—¥å¿—æ•°æ®
  - service_graph.gexfï¼šæœåŠ¡è°ƒç”¨å…³ç³»å›¾
  - dataset_stats.jsonï¼šæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    """)


if __name__ == "__main__":
    main()